{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuqLu/ml-proj/blob/main/YOLOv8_car_tracking_and_counting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "This app uses the `YOLOv8` model for object detection on the road.\n",
        "\n",
        "The objects detected are: humans, cars, bicycles, motorcycles, trucks, and license plates.\n",
        "\n",
        "It uses the `deep-sort-realtime` library for tracking detected cars, bicycles, motorcycles, and trucks in order to count what number of them pass through designated areas on roads.\n",
        "\n",
        "Additionally, it uses a blurring mechanism to anonymize license plates and people detected within the images.\n",
        "\n",
        "To detect vehicles and people, the `YOLOv8m` model was used, which can detect these objects without additional training.\n",
        "\n",
        "The `YOLOv8m` model was also used for license plate detection, but this time it was additionally trained on a set of `1500` license plate images.\n",
        "\n",
        "The performance of the algorithm has been verified not with video showing the road from above, but with car camera video, which makes vehicle detection a bit more difficult due to the smaller angle at which vehicles are visible."
      ],
      "metadata": {
        "id": "Qc0HyQtgv-Mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://github.com/LuqLu/ml-proj/blob/main/car_counting.gif?raw=true)"
      ],
      "metadata": {
        "id": "PiQXd0m9lLAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalation of required packages\n",
        "\n",
        "The `ultralytics` package provides a `YOLO` model which is a high-speed, high-accuracy object detection and image segmentation model.\n",
        "\n",
        "The `deep-sort-realtime` package provides an implementation of `Deep SORT` algorithm that combines a deep learning-based object detector with a `SORT` (Simple Online Realtime Tracking) algorithm."
      ],
      "metadata": {
        "id": "KwUmXmOEEOV7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m96Wauqctv_"
      },
      "outputs": [],
      "source": [
        "%pip install ultralytics\n",
        "%pip install deep-sort-realtime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import of necessary Python libraries and modules"
      ],
      "metadata": {
        "id": "bgBCCKTVEgU8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTB9V-qjdI8X"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import IPython\n",
        "import datetime\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from moviepy.video.io import ffmpeg_tools\n",
        "from IPython.display import Video, display\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import deque\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Config class stores application parameters for fixed values.\n",
        "The `YOLOv8l` model was used for vehicle detection. For vehicle detection we use original `YOLO` model which is pretrained on the `COCO` dataset. `COCO` dataset contains many classes, including `bicycle`, `car`, `motorbike`, `bus`, `truck` therefore this model should correctly detect mentioned objects.\n",
        "\n",
        "The `YOLOv8m` model was used for license plate recognition, which was further trained on a set of `1500` license plate images. To use the trained model (the weights of the model that achieved the best performance on a validation set) we refer to the file `best.pt` in the `weights` directory."
      ],
      "metadata": {
        "id": "Yd432LgfGIPP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhyyWXbgRKjs"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "  \"\"\"\n",
        "  Configuration class for storing application settings and parameters.\n",
        "  \"\"\"\n",
        "\n",
        "  # Path to the main YOLO model file\n",
        "  MODEL = '/content/yolov8m.pt'\n",
        "  # Path to the license plate detection model file\n",
        "  LICENCE_PLATE_MODEL = '/content/drive/MyDrive/yolo/runs/detect/yolov8_licence_plate/weights/best.pt'\n",
        "  # Coordinates of the starting point of line A\n",
        "  START_LINE_A = (10, 1000)\n",
        "  # Coordinates of the ending point of line A\n",
        "  END_LINE_A = (1000, 1000)\n",
        "  # Coordinates of the starting point of line B\n",
        "  START_LINE_B = (1500, 1000)\n",
        "  # Coordinates of the ending point of line B\n",
        "  END_LINE_B = (2360, 1000)\n",
        "  # Coordinates of the starting point of line C\n",
        "  START_LINE_C = (2050, 950)\n",
        "  # Coordinates of the ending point of line C\n",
        "  END_LINE_C = (2400, 950)\n",
        "  # List of class IDs to detect with YOLO model\n",
        "  CLASS_IDS_TO_DETECT = [0, 1, 2, 3, 5, 7]\n",
        "  # Size of the image on which the model will perform the prediction\n",
        "  IMGSZ = (1600, 2560)\n",
        "  # Kernel size for blurring images (Odd number required)\n",
        "  KSIZE = (35, 35)\n",
        "  # Maximum age for objects tracked by the DeepSort tracker\n",
        "  MAX_TRACKER_AGE = 50\n",
        "  # Maximum length of each deque storing tracking points\n",
        "  MAX_DEQUE_LENGHT = 32\n",
        "  # Maximum number of deques used for tracking\n",
        "  MAX_DEQUE_COUNT = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In the VideoTools class we create methods useful for operating on video files\n",
        "Methods:\n",
        "- `get_video` - open a video file specified by the given path\n",
        "- `get_extracted_video` - extracts a portion of a video file and returns a VideoCapture object for the extracted video\n",
        "- `get_video_params` - retrieve such parameters of a video like: frame height and width, fps, frame count\n",
        "- `display_video_info` - display information about the given video\n",
        "- `create_video_writer` - create a VideoWriter object to write processed video frames"
      ],
      "metadata": {
        "id": "vJ4mLPvWM8sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoTools:\n",
        "  \"\"\"\n",
        "  A utility class for working with video files.\n",
        "\n",
        "  This class provides static methods to perform various operations\n",
        "  on video files, such as loading videos and extracting frames.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def get_video(path):\n",
        "    \"\"\"\n",
        "    Open a video file specified by the given path.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the video file.\n",
        "\n",
        "    Returns:\n",
        "        cv2.VideoCapture: VideoCapture object for passed video.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the video file cannot be opened.\n",
        "    \"\"\"\n",
        "    file = cv2.VideoCapture(path)\n",
        "    if not file.isOpened():\n",
        "      raise ValueError(\"Error: Video file cannot be opened.\")\n",
        "    return file\n",
        "\n",
        "  @staticmethod\n",
        "  def get_extracted_video(path, start_time, end_time, output_path=None):\n",
        "    \"\"\"\n",
        "    Extracts a portion of a video file and returns a VideoCapture object for the extracted video.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the input video file.\n",
        "        start_time (float): Start time of the portion to extract (in seconds).\n",
        "        end_time (float): End time of the portion to extract (in seconds).\n",
        "        output_path (str, optional): Path to save the extracted video. If not specified, a default path is used.\n",
        "\n",
        "    Returns:\n",
        "        cv2.VideoCapture: VideoCapture object for the extracted video.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the extracted video file cannot be opened.\n",
        "    \"\"\"\n",
        "    if output_path is None:\n",
        "      output_path = '/content/extracted_video.mp4'\n",
        "\n",
        "    # Extract a portion of the video and save it as a new file\n",
        "    ffmpeg_tools.ffmpeg_extract_subclip(path, start_time, end_time, output_path)\n",
        "\n",
        "    # Open the extracted video file\n",
        "    file = cv2.VideoCapture(output_path)\n",
        "    if not file.isOpened():\n",
        "      raise ValueError(\"Error: Failed to open extracted video file.\")\n",
        "\n",
        "    # Delete the extracted video file to avoid cluttering the file system\n",
        "    os.remove(output_path)\n",
        "\n",
        "    return file\n",
        "\n",
        "  @staticmethod\n",
        "  def get_video_params(video):\n",
        "    \"\"\"\n",
        "    Retrieve various parameters of a video.\n",
        "\n",
        "    Args:\n",
        "        video (cv2.VideoCapture): VideoCapture object representing the video.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the height, width, frames per second (fps), and total frame count of the video.\n",
        "    \"\"\"\n",
        "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    return height, width, fps, frame_count\n",
        "\n",
        "  @staticmethod\n",
        "  def display_video_info(video):\n",
        "    \"\"\"\n",
        "    Display information about the given video.\n",
        "\n",
        "    Args:\n",
        "        video (cv2.VideoCapture): VideoCapture object representing the video.\n",
        "    \"\"\"\n",
        "    height, width, fps, frame_count = VideoTools.get_video_params(video)\n",
        "    seconds = round(frame_count / fps)\n",
        "    video_time = datetime.timedelta(seconds=seconds)\n",
        "    print(f'Original video dim: {(height, width)}')\n",
        "    print(f'Video fps: {fps}')\n",
        "    print(f'Number of frames: {frame_count}')\n",
        "    print(f'Duration in seconds: {seconds}')\n",
        "    print(f'Video time: {video_time}')\n",
        "\n",
        "  @staticmethod\n",
        "  def create_video_writer(video, output_file_path=None):\n",
        "    \"\"\"\n",
        "    Create a VideoWriter object to write processed video frames.\n",
        "\n",
        "    Args:\n",
        "        video (cv2.VideoCapture): VideoCapture object representing the video.\n",
        "        output_file_path (str): The path to save the processed video.\n",
        "\n",
        "    Returns:\n",
        "        cv2.VideoWriter: A VideoWriter object for writing video frames.\n",
        "    \"\"\"\n",
        "    if output_file_path is None:\n",
        "      output_file_path = '/content/output_video.mp4'\n",
        "\n",
        "    height, width, fps, frame_count = VideoTools.get_video_params(video)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    image_size = (width, height)\n",
        "    return cv2.VideoWriter(output_file_path, fourcc, fps, image_size)"
      ],
      "metadata": {
        "id": "wDYOrSmjyzyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The GeometricUtils class contains a method for calculating midpoint for given two points.\n",
        "Methods:\n",
        "- `calculate_midpoint` - calculate the midpoint between two points\n",
        "\n",
        "We will use this method to determine where in the line we will put the number of vehicles that have passed the line."
      ],
      "metadata": {
        "id": "lyugClY4cQon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeometricUtils:\n",
        "  \"\"\"\n",
        "  A utility class for performing geometric calculations.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def calculate_midpoint(a_point, b_point):\n",
        "    \"\"\"\n",
        "    Calculate the midpoint between two points.\n",
        "\n",
        "    Args:\n",
        "        a_point (tuple): Coordinates of the first point (x1, y1).\n",
        "        b_point (tuple): Coordinates of the second point (x2, y2).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Coordinates of the midpoint (xc, yc).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input format is invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      x1, y1 = a_point\n",
        "      x2, y2 = b_point\n",
        "      xc = (x1 + x2) // 2\n",
        "      yc = (y1 + y2) // 2\n",
        "      return xc, yc\n",
        "    except (TypeError, ValueError):\n",
        "      raise ValueError('Invalid input format. Expected tuples with two elements.')"
      ],
      "metadata": {
        "id": "BPAckwlSbLOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In the ImageProcessor class we create methods useful for operating on images (single frames of a video file)\n",
        "Methods:\n",
        "- `blur_picture` - blurs regions of an image based on detections (we want to blur license plates and images of people in the processed video)\n",
        "- `draw_bbox` - draw a bounding box on an image along with class name and track id\n",
        "- `add_description` - add descriptions onto an image"
      ],
      "metadata": {
        "id": "nC6X-AbHNjVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageProcessor:\n",
        "  \"\"\"\n",
        "  A utility class for image processing operations.\n",
        "\n",
        "  This class provides static methods for performing various image processing tasks,\n",
        "  including blurring, calculating midpoints, and adding descriptions to images.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def blur_picture(img, detections):\n",
        "    \"\"\"\n",
        "    Blurs regions of an image based on detections.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): Input image.\n",
        "        detections (list): List of dictionaries representing detected objects.\n",
        "            Each dictionary should contain a 'box' key representing the bounding box coordinates.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Image with blurred regions.\n",
        "    \"\"\"\n",
        "    for obj in detections:\n",
        "      x1,y1,x2,y2 = obj['box']\n",
        "      x1, y1, x2, y2 = round(x1), round(y1), round(x2), round(y2)\n",
        "      roi = img[y1:y2, x1:x2]\n",
        "      blurred_roi = cv2.GaussianBlur(roi, Config.KSIZE, 0)\n",
        "      img[y1:y2, x1:x2] = blurred_roi\n",
        "\n",
        "    return img\n",
        "\n",
        "  @staticmethod\n",
        "  def draw_lines(img):\n",
        "    \"\"\"\n",
        "    Draws three lines on the given image and overlays it with a copy of the image.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): The image on which the lines will be drawn.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The modified image with lines drawn and overlaid.\n",
        "    \"\"\"\n",
        "    # Create a copy of the image to use as an overlay\n",
        "    overlay = img.copy()\n",
        "\n",
        "    # Draw the lines using predefined start and end points\n",
        "    cv2.line(img, Config.START_LINE_A, Config.END_LINE_A, (0, 255, 0), 15)\n",
        "    cv2.line(img, Config.START_LINE_B, Config.END_LINE_B, (255, 0, 0), 15)\n",
        "    cv2.line(img, Config.START_LINE_C, Config.END_LINE_C, (0, 0, 255), 15)\n",
        "\n",
        "    # Overlay the original image with the copy using addWeighted to blend them\n",
        "    img = cv2.addWeighted(overlay, 0.5, img, 0.5, 0)\n",
        "\n",
        "    return img\n",
        "\n",
        "  @staticmethod\n",
        "  def draw_bbox(img, model, bbox, track_id, class_id):\n",
        "    \"\"\"\n",
        "    Draw a bounding box on an image along with class name and track ID.\n",
        "\n",
        "    Args:\n",
        "        img: The image on which to draw the bounding box.\n",
        "        model: Model object capable of making predictions.\n",
        "        bbox: A list containing the coordinates of the bounding box in the format [x1, y1, x2, y2].\n",
        "        track_id: The identifier of the tracked object.\n",
        "        class_id: The identifier of the predicted class.\n",
        "\n",
        "    Returns:\n",
        "        img: The image with the bounding box, class name, and track id drawn on it.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If class_id is out of range.\n",
        "    \"\"\"\n",
        "    if img is None or bbox is None or len(bbox) != 4:\n",
        "      raise ValueError('Invalid input: img and bbox must be provided and bbox must be of length 4.')\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "\n",
        "    # Get class name\n",
        "    class_names = model.model.names\n",
        "    if class_id < 0 or class_id >= len(class_names):\n",
        "      raise ValueError('Invalid class_id: class_id out of range.')\n",
        "    class_name = class_names[class_id]\n",
        "\n",
        "    # Create a list of random colors to represent each class\n",
        "    np.random.seed(10)  # To get the same colors\n",
        "    colors = np.random.randint(0, 255, size=(len(class_names), 3))\n",
        "\n",
        "    # Get the color associated with the class name\n",
        "    color = tuple(map(int, colors[class_id]))\n",
        "\n",
        "    # Draw bounding box\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
        "\n",
        "    # Draw class name and track id\n",
        "    text = str(track_id) + \" - \" + class_name\n",
        "    cv2.rectangle(img, (x1 - 1, y1 - 20), (x1 + len(text) * 12, y1), color, -1)\n",
        "    cv2.putText(img, text, (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "    return img\n",
        "\n",
        "  @staticmethod\n",
        "  def add_description(img, model, all_counters, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=1, color=(255, 255, 255), thickness=2):\n",
        "    \"\"\"\n",
        "    Add descriptions and counters onto an image.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): Input image.\n",
        "        model: Model object capable of making predictions.\n",
        "        all_counters (dict): A dictionary containing counts of the total number of vehicles and individual types of vehicles\n",
        "            for each of the zones under consideration\n",
        "        font (int): Font type. Default is cv2.FONT_HERSHEY_SIMPLEX.\n",
        "        font_scale (float): Font scale factor. Default is 0.5.\n",
        "        color (tuple): Text color specified as a tuple of three integers representing BGR values. Default is (255, 255, 255) (white).\n",
        "        thickness (int): Thickness of the text. Default is 2.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Image with descriptions and counters added.\n",
        "    \"\"\"\n",
        "    texts = ['A', 'B', 'C']\n",
        "    text_positions = [Config.START_LINE_A, Config.START_LINE_B, Config.START_LINE_C]\n",
        "    counters = [all_counters['counter_A'], all_counters['counter_B'], all_counters['counter_C']]\n",
        "    counter_positions = [\n",
        "        GeometricUtils.calculate_midpoint(Config.START_LINE_A, Config.END_LINE_A),\n",
        "        GeometricUtils.calculate_midpoint(Config.START_LINE_B, Config.END_LINE_B),\n",
        "        GeometricUtils.calculate_midpoint(Config.START_LINE_C, Config.END_LINE_C)\n",
        "    ]\n",
        "\n",
        "    # Draw the total number of vehicles passing the lines\n",
        "    for text, text_position, counter, counter_position in zip(texts, text_positions, counters, counter_positions):\n",
        "      cv2.putText(img, text, text_position, font, font_scale, color, thickness)\n",
        "      cv2.putText(img, str(counter), counter_position, font, font_scale, color, thickness)\n",
        "\n",
        "    # Summary of the type of vehicle and the number of its occurrences\n",
        "    dict_classes = model.model.names\n",
        "    result_A = [f'{dict_classes[k]}: {i}' for k, i in all_counters['vehicle_counter_A'].items()]\n",
        "    result_B = [f'{dict_classes[k]}: {i}' for k, i in all_counters['vehicle_counter_B'].items()]\n",
        "    result_C = [f'{dict_classes[k]}: {i}' for k, i in all_counters['vehicle_counter_C'].items()]\n",
        "\n",
        "    # Draw the counting of type of vehicles\n",
        "    font_color = (255, 255, 0)\n",
        "    margine = 70 # distance from the top edge of the image\n",
        "\n",
        "    cv2.putText(img, 'Line A', (Config.START_LINE_A[0] + 20, margine), cv2.FONT_HERSHEY_TRIPLEX, 1, font_color, 1)\n",
        "    cv2.putText(img, 'Line B', (Config.START_LINE_B[0], margine), cv2.FONT_HERSHEY_TRIPLEX, 1, font_color, 1)\n",
        "    cv2.putText(img, 'Line C', (Config.START_LINE_C[0] + 100, margine), cv2.FONT_HERSHEY_TRIPLEX, 1, font_color, 1)\n",
        "\n",
        "    margine = 90\n",
        "    for i in range(len(result_A)):\n",
        "      margine +=30\n",
        "      cv2.putText(img, f'{result_A[i]}', (Config.START_LINE_A[0] + 20, margine), cv2.FONT_HERSHEY_TRIPLEX, 1, font_color, 1)\n",
        "      cv2.putText(img, f'{result_B[i]}', (Config.START_LINE_B[0], margine), cv2.FONT_HERSHEY_TRIPLEX, 1, font_color, 1)\n",
        "      cv2.putText(img, f'{result_C[i]}', (Config.START_LINE_C[0] + 100, margine), cv2.FONT_HERSHEY_TRIPLEX, 1, font_color, 1)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "D7ovsBG1y1K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The main class that performs detection and tracking of the objects considered above. In addition to its own methods, it uses methods from the above classes: VideoTools, ImageProcessor.\n",
        "Methods:\n",
        "- `load_model` - load a model from a given path\n",
        "- `predict` - predict the classes of objects in the given image using the provided model\n",
        "- `get_bbox_params` - extract bounding box parameters (coordinates, class labels, confidences) from the results\n",
        "- `convert_to_tracker_format` - convert detections to the format expected by the tracker\n",
        "- `count_vehicles` - counts vehicles based on the bounding box and track id\n",
        "- `track_detect` - perform object tracking on the given image using the provided detections and tracker"
      ],
      "metadata": {
        "id": "bDtaOfYiF-my"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectDetection:\n",
        "  \"\"\"\n",
        "  A class for performing object detection and tracking in videos.\n",
        "\n",
        "  This class encapsulates functionality for loading object detection models,\n",
        "  predicting object bounding boxes in video frames, converting detections to\n",
        "  tracker-compatible format, counting detected objects, and tracking objects\n",
        "  across frames in a video stream.\n",
        "\n",
        "  Attributes:\n",
        "      video: The input video file or video stream for object detection and tracking.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, video):\n",
        "    \"\"\"\n",
        "    Initialize ObjectDetection object.\n",
        "\n",
        "    Args:\n",
        "        video: Video object.\n",
        "    \"\"\"\n",
        "    self.video = video\n",
        "    self.model = self.load_model(Config.MODEL)\n",
        "    self.licence_plate_model = self.load_model(Config.LICENCE_PLATE_MODEL)\n",
        "    self.points = [deque(maxlen=Config.MAX_DEQUE_LENGHT) for _ in range(Config.MAX_DEQUE_COUNT)]\n",
        "    self.counter_A = 0\n",
        "    self.counter_B = 0\n",
        "    self.counter_C = 0\n",
        "    self.vehicle_counter_A = dict.fromkeys(Config.CLASS_IDS_TO_DETECT[1:], 0)\n",
        "    self.vehicle_counter_B = dict.fromkeys(Config.CLASS_IDS_TO_DETECT[1:], 0)\n",
        "    self.vehicle_counter_C = dict.fromkeys(Config.CLASS_IDS_TO_DETECT[1:], 0)\n",
        "\n",
        "    self.tracked_ids = set()\n",
        "\n",
        "  def load_model(self, path):\n",
        "    \"\"\"\n",
        "    Load a model from the given path.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the model file.\n",
        "\n",
        "    Returns:\n",
        "        model: The loaded model object.\n",
        "    \"\"\"\n",
        "    model = YOLO(path)\n",
        "    return model\n",
        "\n",
        "  def predict(self, img, model, classes, verbose=False):\n",
        "    \"\"\"\n",
        "    Predict the classes of objects in the given image using the provided model.\n",
        "\n",
        "    Args:\n",
        "        img: Input image for prediction.\n",
        "        model: Model object capable of making predictions.\n",
        "        classes (list): List of classes to predict by the model.\n",
        "        verbose (bool, optional): Verbosity mode. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        results: Results of the prediction.\n",
        "    \"\"\"\n",
        "    results = model.predict(img, imgsz=Config.IMGSZ, classes=classes, device=0, verbose=verbose)\n",
        "    return results\n",
        "\n",
        "  def get_bbox_params(self, results):\n",
        "    \"\"\"\n",
        "    Extract bounding box parameters (coordinates, class labels, confidences) from the results.\n",
        "\n",
        "    Args:\n",
        "        results: The results of object detection.\n",
        "\n",
        "    Returns:\n",
        "        detections: A list of dictionaries containing bounding box parameters.\n",
        "            Each dictionary contains keys 'box', 'class', and 'conf'.\n",
        "    \"\"\"\n",
        "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "    classes = results[0].boxes.cls.cpu().numpy()\n",
        "    confs = results[0].boxes.conf.cpu().numpy()\n",
        "\n",
        "    detections = []\n",
        "    for box, cls, conf in zip(boxes, classes, confs):\n",
        "      detection = {\n",
        "          'box': box,\n",
        "          'class': cls,\n",
        "          'conf': conf\n",
        "      }\n",
        "      detections.append(detection)\n",
        "    return detections\n",
        "\n",
        "  def convert_to_tracker_format(self, detections):\n",
        "    \"\"\"\n",
        "    Convert detections to the format expected by the tracker.\n",
        "\n",
        "    Args:\n",
        "        detections (list): A list of dictionaries containing detection information.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of detections in the format [[xmin, ymin, w, h], confidence, class_id].\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for obj in detections:\n",
        "      xmin, ymin, xmax, ymax = obj['box']\n",
        "      confidence = obj['conf']\n",
        "      class_id = int(obj['class'])\n",
        "      bbox_width = xmax - xmin\n",
        "      bbox_height = ymax - ymin\n",
        "      results.append([[xmin, ymin, bbox_width, bbox_height], confidence, class_id])\n",
        "    return results\n",
        "\n",
        "  def count_vehicles(self, img, bbox, track_id, class_id):\n",
        "    \"\"\"\n",
        "    Counts vehicles based on the bounding box and track id.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): Input image.\n",
        "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2).\n",
        "        track_id (int): Unique identifier for the object track.\n",
        "        class_id (int): Identifier for the object class.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Image with added centres of bounding boxes.\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "    # Determination of the interior point of bounding box\n",
        "    interior_x = int((x1 + x2) / 2)\n",
        "    center_y = int((y1 + y2) / 2)\n",
        "    if class_id == 1:\n",
        "      interior_y = y2\n",
        "    else:\n",
        "      interior_y = int((center_y + y2) / 2)\n",
        "\n",
        "    # In general, you can set any percentage of the bounding box height.\n",
        "    # interior_y = (y2 - y1) * 0.75 + y1\n",
        "\n",
        "    # Append the interior point of the current object to the points list\n",
        "    self.points[track_id].append((interior_x, interior_y))\n",
        "\n",
        "    # Get the last point from the points list and draw it\n",
        "    last_point_x = self.points[track_id][0][0]\n",
        "    last_point_y = self.points[track_id][0][1]\n",
        "\n",
        "    # Count the number of vehicles passing the lines\n",
        "    # Check the condition whether the object has crossed the line\n",
        "    if interior_y > Config.START_LINE_A[1] and Config.START_LINE_A[0] < interior_x < Config.END_LINE_A[0] and last_point_y < Config.START_LINE_A[1]:\n",
        "      self.counter_A += 1\n",
        "      self.vehicle_counter_A[class_id] += 1\n",
        "      self.points[track_id].clear()\n",
        "    elif interior_y < Config.START_LINE_B[1] and Config.START_LINE_B[0] < interior_x < Config.END_LINE_B[0] and last_point_y > Config.START_LINE_B[1]:\n",
        "      self.counter_B += 1\n",
        "      self.vehicle_counter_B[class_id] += 1\n",
        "      self.points[track_id].clear()\n",
        "    elif interior_y < Config.START_LINE_C[1] and Config.START_LINE_C[0] < interior_x < Config.END_LINE_C[0] and last_point_y > Config.START_LINE_C[1]:\n",
        "      self.counter_C += 1\n",
        "      self.vehicle_counter_C[class_id] += 1\n",
        "      self.points[track_id].clear()\n",
        "    return img\n",
        "\n",
        "  def track_detect(self, img, detections, tracker):\n",
        "    \"\"\"\n",
        "    Perform object tracking on the given image using the provided detections and tracker.\n",
        "\n",
        "    Args:\n",
        "        img: The input image for object tracking.\n",
        "        detections (list): A list of detections in the format [[xmin, ymin, w, h], confidence, class_id].\n",
        "        tracker: The object tracker used for tracking.\n",
        "\n",
        "    Returns:\n",
        "        img: The input image with object tracking results.\n",
        "    \"\"\"\n",
        "    # Update tracker with the new detections\n",
        "    tracks = tracker.update_tracks(detections, frame=img)\n",
        "\n",
        "    # Loop over the tracks\n",
        "    for track in tracks:\n",
        "      if not track.is_confirmed() or track.time_since_update > 1 or track.det_class == 0:\n",
        "        continue\n",
        "      # Get the bounding box of the object, the name of the object, and the track id\n",
        "      bbox = track.to_tlbr()\n",
        "      track_id = int(track.track_id)\n",
        "      class_id = track.det_class\n",
        "\n",
        "      # Draw the bounding box of the object and the track id\n",
        "      img = ImageProcessor.draw_bbox(img, self.model, bbox, track_id, class_id)\n",
        "\n",
        "      img = self.count_vehicles(img, bbox, track_id, class_id)\n",
        "\n",
        "    counters = {\n",
        "        'counter_A': self.counter_A,\n",
        "        'counter_B': self.counter_B,\n",
        "        'counter_C': self.counter_C,\n",
        "        'vehicle_counter_A': self.vehicle_counter_A,\n",
        "        'vehicle_counter_B': self.vehicle_counter_B,\n",
        "        'vehicle_counter_C': self.vehicle_counter_C\n",
        "    }\n",
        "\n",
        "    img = ImageProcessor.add_description(img, self.model, counters)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "  def __call__(self):\n",
        "    \"\"\"\n",
        "    Process the video by reading frames, making predictions, processing frames, tracking objects, and saving the output video.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    tracker = DeepSort(max_age=Config.MAX_TRACKER_AGE)\n",
        "    output_video = VideoTools.create_video_writer(self.video)\n",
        "\n",
        "    frame_count = int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    for _ in tqdm(range(frame_count)):\n",
        "      success, frame = self.video.read()\n",
        "      assert success, 'Failed to read a frame'\n",
        "\n",
        "      # Draw lines designating vehicle counting locations\n",
        "      frame = ImageProcessor.draw_lines(frame)\n",
        "\n",
        "      # Detect and blur licence plates in the frame\n",
        "      licence_plate_results = self.predict(frame, self.licence_plate_model, None, False)\n",
        "      licence_plate_boxes = self.get_bbox_params(licence_plate_results)\n",
        "      frame = ImageProcessor.blur_picture(frame, licence_plate_boxes)\n",
        "\n",
        "      # Detect other object\n",
        "      results = self.predict(frame, self.model, Config.CLASS_IDS_TO_DETECT, False)\n",
        "      boxes = self.get_bbox_params(results)\n",
        "\n",
        "      # Blur person in the frame\n",
        "      person_boxes = [box for box in boxes if box['class'] == 0]\n",
        "      frame = ImageProcessor.blur_picture(frame, person_boxes)\n",
        "\n",
        "      tracker_boxes = self.convert_to_tracker_format(boxes)\n",
        "      frame = self.track_detect(frame, tracker_boxes, tracker)\n",
        "\n",
        "      # Save transformed frames in an output video\n",
        "      output_video.write(frame)\n",
        "\n",
        "    # Release resources\n",
        "    output_video.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "QlI2O-mh9nLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launching the application for the given video file"
      ],
      "metadata": {
        "id": "EcQJDM9md4yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = 'path_to_video_file'\n",
        "video = VideoTools.get_video(video_path)\n",
        "# Or you can use just a portion of the video\n",
        "# video = VideoTools.get_extracted_video(video_path, 90, 120)\n",
        "VideoTools.display_video_info(video)\n",
        "detection = ObjectDetection(video)\n",
        "detection()"
      ],
      "metadata": {
        "id": "zr8dOMvtEpdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z9nzpBuM4ig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "772cf59b682f4e9da0cc6135320ebaab",
            "521ec57922934e19bc26cf2b7c245c3b",
            "688ec8f0a13f46aea47bd015d48546f1",
            "499c8ec3c69a40adbe94aefaee9ab05e",
            "c75ba00719d544ac90bafdc703956028",
            "281ab2ec30dc4e75a623921c149afff7",
            "c5ae221e57d84168a70033bded1f8df6",
            "a926d38ade61469bb568f69f671bcba8",
            "af40fe61d81d476d895c9fdde2db52cc",
            "af00167880b5445381b087680fd5d71e",
            "b99d1a8df58b43759a6ab0dc93cb1082"
          ]
        },
        "outputId": "f509a99c-816f-42f3-bbc5-fe8b03b432d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Original video dim: (1600, 2560)\n",
            "Video fps: 30.0\n",
            "Number of frames: 870\n",
            "Duration in seconds: 29\n",
            "Video time: 0:00:29\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8m.pt to '/content/yolov8m.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 244MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/870 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "772cf59b682f4e9da0cc6135320ebaab"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "video_path = '/content/drive/MyDrive/yolo/20230521150750_000050.MP4'\n",
        "\n",
        "video = VideoTools.get_extracted_video(video_path, 94, 123)\n",
        "#video = VideoTools.get_video(video_path) # Or you can use the whole video\n",
        "VideoTools.display_video_info(video)\n",
        "detection = ObjectDetection(video)\n",
        "detection()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1T-Y3d8bAXLZg2eGfIz7xE7R8X739AztM",
      "authorship_tag": "ABX9TyOgJVG2fvhXBxzCYfZfGyXS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "772cf59b682f4e9da0cc6135320ebaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_521ec57922934e19bc26cf2b7c245c3b",
              "IPY_MODEL_688ec8f0a13f46aea47bd015d48546f1",
              "IPY_MODEL_499c8ec3c69a40adbe94aefaee9ab05e"
            ],
            "layout": "IPY_MODEL_c75ba00719d544ac90bafdc703956028"
          }
        },
        "521ec57922934e19bc26cf2b7c245c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_281ab2ec30dc4e75a623921c149afff7",
            "placeholder": "​",
            "style": "IPY_MODEL_c5ae221e57d84168a70033bded1f8df6",
            "value": "100%"
          }
        },
        "688ec8f0a13f46aea47bd015d48546f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a926d38ade61469bb568f69f671bcba8",
            "max": 870,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af40fe61d81d476d895c9fdde2db52cc",
            "value": 870
          }
        },
        "499c8ec3c69a40adbe94aefaee9ab05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af00167880b5445381b087680fd5d71e",
            "placeholder": "​",
            "style": "IPY_MODEL_b99d1a8df58b43759a6ab0dc93cb1082",
            "value": " 870/870 [09:28&lt;00:00,  1.48it/s]"
          }
        },
        "c75ba00719d544ac90bafdc703956028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "281ab2ec30dc4e75a623921c149afff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5ae221e57d84168a70033bded1f8df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a926d38ade61469bb568f69f671bcba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af40fe61d81d476d895c9fdde2db52cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af00167880b5445381b087680fd5d71e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b99d1a8df58b43759a6ab0dc93cb1082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
